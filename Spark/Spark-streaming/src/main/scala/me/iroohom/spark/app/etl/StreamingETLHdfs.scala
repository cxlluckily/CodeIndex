package me.iroohom.spark.app.etl

import me.iroohom.spark.app.StreamingContextUtils
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.DStream
import org.lionsoul.ip2region.{DbConfig, DbSearcher}

object StreamingETLHdfs {
  def main(args: Array[String]): Unit = {
    val ssc: StreamingContext = StreamingContextUtils.getStreamingContext(this.getClass, 10)

    val kafkaDStream: DStream[ConsumerRecord[String, String]] = StreamingContextUtils.consumerKafka(ssc, "search-log-topic")

    //Return a new DStream in which each RDD is generated by applying a function
    val etlDStream: DStream[String] = kafkaDStream.transform { rdd =>
      //针对每个分区操作
      rdd.mapPartitions { iter =>
        val dbSearcher = new DbSearcher(new DbConfig(), "dataset/ip2region.db")
        iter.map { record =>
          //获取Message消息
          val message = record.value()
          //提取IP地址
          val ipValue = message.split(",")(1)
          //传递IP解析地址
          val dataBlock = dbSearcher.btreeSearch(ipValue)
          //获取地区
          val region: String = dataBlock.getRegion
          val Array(_, _, province, city, _) = region.split("\\|")

          s"${message},${province},${city}"
        }
      }
    }

    /**
     * 保存到HDFS
     */
    etlDStream.foreachRDD { (rdd, time) => {
      if (!rdd.isEmpty()) {
        rdd
          .coalesce(1)
          .saveAsTextFile(s"datas/spark/search-logs-${time.milliseconds}")
      }
    }
    }


    ssc.start()
    ssc.awaitTermination()
    ssc.stop(stopSparkContext = true, stopGracefully = true)
  }
}
